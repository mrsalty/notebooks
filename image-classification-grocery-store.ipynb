{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","toc_visible":true,"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mrsalty/image-classification-of-grocery-store-with-dl?scriptVersionId=241749257\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Image Classification of Grocery Store with Deep Learning","metadata":{"id":"MNBgGYg_lpVN"}},{"cell_type":"markdown","source":"The goal of this notebook is to implement a neural network that classifies smartphone pictures of products found in grocery stores.\nWe will divide in 2 steps:\n1- implement from scratch a baseline neural network for image classification.\n2- Fine-Tune ResNet-18 on the dataset provided.\n","metadata":{"id":"b5NCd_YwSAc4"}},{"cell_type":"markdown","source":"## Preliminaries: the dataset\n\n\n\nThe dataset contains natural images of products taken with a smartphone camera in different grocery stores:\n\n\n\n<p align=\"center\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Granny-Smith.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Pink-Lady.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Lemon.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Banana.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Vine-Tomato.jpg\" width=\"150\">\n</p>\n\n<p align=\"center\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Yellow-Onion.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Green-Bell-Pepper.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Arla-Standard-Milk.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Oatly-Natural-Oatghurt.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Alpro-Fresh-Soy-Milk.jpg\" width=\"150\">\n</p>\n\n\nThe products belong to the following 43 classes:\n\n```\n\n0.  Apple\n1.  Avocado\n2.  Banana\n3.  Kiwi\n4.  Lemon\n5.  Lime\n6.  Mango\n7.  Melon\n8.  Nectarine\n9.  Orange\n10. Papaya\n11. Passion-Fruit\n12. Peach\n13. Pear\n14. Pineapple\n15. Plum\n16. Pomegranate\n17. Red-Grapefruit\n18. Satsumas\n19. Juice\n20. Milk\n21. Oatghurt\n22. Oat-Milk\n23. Sour-Cream\n24. Sour-Milk\n25. Soyghurt\n26. Soy-Milk\n27. Yoghurt\n28. Asparagus\n29. Aubergine\n30. Cabbage\n31. Carrots\n32. Cucumber\n33. Garlic\n34. Ginger\n35. Leek\n36. Mushroom\n37. Onion\n38. Pepper\n39. Potato\n40. Red-Beet\n41. Tomato\n42. Zucchini\n```\n\nThe dataset is split into training (`train`), validation (`val`), and test (`test`) set.","metadata":{"id":"dVTQUJ4uYH1w"}},{"cell_type":"markdown","source":"Let's download the dataset from GitHub","metadata":{"id":"1pdrmJRnJPd8"}},{"cell_type":"code","source":"!git clone https://github.com/marcusklasson/GroceryStoreDataset.git","metadata":{"id":"POMX_3x-_bZI","outputId":"d150ed90-d611-4876-a7bb-78b7e8a261ad","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:42.182185Z","iopub.execute_input":"2025-05-25T10:54:42.182767Z","iopub.status.idle":"2025-05-25T10:54:46.521591Z","shell.execute_reply.started":"2025-05-25T10:54:42.182743Z","shell.execute_reply":"2025-05-25T10:54:46.520709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nfrom PIL import Image\nfrom torch import Tensor\nfrom torch.utils.data import Dataset\nfrom typing import List, Tuple","metadata":{"id":"215xcPvISAc5","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:46.523232Z","iopub.execute_input":"2025-05-25T10:54:46.523496Z","iopub.status.idle":"2025-05-25T10:54:50.815225Z","shell.execute_reply.started":"2025-05-25T10:54:46.523473Z","shell.execute_reply":"2025-05-25T10:54:50.814452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GroceryStoreDataset(Dataset):\n    def __init__(self, split: str, transform=None) -> None:\n        super().__init__()\n        self.root = Path(\"GroceryStoreDataset/dataset\")\n        self.split = split\n        self.paths, self.labels = self.read_file()\n        self.transform = transform\n\n    def __len__(self) -> int:\n        return len(self.labels)\n\n    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n        img = Image.open(self.root / self.paths[idx])\n        label = self.labels[idx]\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label\n\n    def read_file(self) -> Tuple[List[str], List[int]]:\n        paths = []\n        labels = []\n        with open(self.root / f\"{self.split}.txt\") as f:\n            for line in f:\n                # path, fine-grained class, coarse-grained class\n                path, _, label = line.replace(\"\\n\", \"\").split(\", \")\n                paths.append(path), labels.append(int(label))\n        return paths, labels\n\n    def get_num_classes(self) -> int:\n        return max(self.labels) + 1","metadata":{"id":"jROSO2qVDxdD","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:50.816078Z","iopub.execute_input":"2025-05-25T10:54:50.816722Z","iopub.status.idle":"2025-05-25T10:54:50.822885Z","shell.execute_reply.started":"2025-05-25T10:54:50.816696Z","shell.execute_reply":"2025-05-25T10:54:50.822144Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part 1: design our own network\n\nWe will implement our own convolutional neural network for image classification and train it on `GroceryStoreDataset`. Our aim is to reach a classification accuracy on the **validation** split of **around 60%**.","metadata":{"id":"yBch3dpwNSsW"}},{"cell_type":"markdown","source":"## Part 2: ResNet-18\n\nWe will use a pretrained **ResNet-18** model on `GroceryStoreDataset` using PyTorch. We will approach in 2 steps:\n\n1. First, fine-tune the Resnet-18 with the same training hyperparameters used for our best model in the first step.\n\n1. Second, we will tweak the training hyperparameters in order to increase the accuracy on the validation split of `GroceryStoreDataset`. Our aim is to  obtain a classification accuracy on the **validation** split **between 80 and 90%**.","metadata":{"id":"gkWEqSPoUIL3"}},{"cell_type":"markdown","source":"# Part 1: Design our own network","metadata":{"id":"rAPMzaBQRUtG"}},{"cell_type":"markdown","source":"We'll begin with a basic Convolutional Neural Network and progressively enhance its architecture, we'll analyze its characteristics and iteratively introduce new components, explaining the reasoning behind each addition. Finally, we'll review the training results of all models to draw insights from the observed outcomes.","metadata":{"id":"0vcWZrInSAc6"}},{"cell_type":"code","source":"import torch\nprint(\"Using torch\", torch.__version__)","metadata":{"id":"D5TrSHboRc_M","outputId":"1723e7bb-f7fc-4268-81cd-30436a14c8f2","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:50.824189Z","iopub.execute_input":"2025-05-25T10:54:50.824386Z","iopub.status.idle":"2025-05-25T10:54:50.842495Z","shell.execute_reply.started":"2025-05-25T10:54:50.824371Z","shell.execute_reply":"2025-05-25T10:54:50.841982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport time\n\nfrom torch.optim import Adam\nfrom torch.utils.data import random_split, DataLoader\nfrom torchvision import datasets, models, transforms\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import accuracy_score\nfrom torchvision.models import ResNet18_Weights\n\ndef fix_random(seed: int) -> None:\n    \"\"\"Fix all the possible sources of randomness.\n    Args:\n        seed: the seed to use.\n\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nfix_random(42)","metadata":{"id":"7z_tt6PmRyui","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:50.843318Z","iopub.execute_input":"2025-05-25T10:54:50.843492Z","iopub.status.idle":"2025-05-25T10:54:55.35492Z","shell.execute_reply.started":"2025-05-25T10:54:50.843477Z","shell.execute_reply":"2025-05-25T10:54:55.354216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gpu_avail = torch.cuda.is_available()\nprint(f\"Is the GPU available? {gpu_avail}\")","metadata":{"id":"2M8PCvNfSEDT","outputId":"3ad0c019-eca0-48ea-d150-80beec4c51b2","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:55.355749Z","iopub.execute_input":"2025-05-25T10:54:55.356169Z","iopub.status.idle":"2025-05-25T10:54:55.45893Z","shell.execute_reply.started":"2025-05-25T10:54:55.356143Z","shell.execute_reply":"2025-05-25T10:54:55.458084Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## \\> Step 1: Simple CNN Baseline\n\nWe’ll start with a very basic CNN architecture with only a few layers:\n\n- Single convolutional layer with 16 filters, followed by a ReLU activation.\n- Max pooling layer to reduce spatial dimensions.\n- Fully Connected layer to map features to class scores.","metadata":{"id":"cGoCki_u9dTY"}},{"cell_type":"code","source":"class GroceryNetV1(nn.Module):\n\n    def __init__(self, num_classes):\n        super(GroceryNetV1, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc = nn.Linear(16 * 64 * 64, num_classes)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x","metadata":{"id":"54DG9tly9LCf","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:55.459777Z","iopub.execute_input":"2025-05-25T10:54:55.460024Z","iopub.status.idle":"2025-05-25T10:54:55.488039Z","shell.execute_reply.started":"2025-05-25T10:54:55.460004Z","shell.execute_reply":"2025-05-25T10:54:55.48732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## \\> Step 2: Stacking More Convolutional Layers\n\nTo capture more complex patterns, we’ll add 4 Convolutional Layers with increased filters. It was empirically observed that, given the limited size of the GroceryStoreDataset dataset, stacking more than 5 Convolutional Layers wouldn't significantly impact performance.","metadata":{"id":"9SaQ3xqR9sSr"}},{"cell_type":"code","source":"class GroceryNetV2(nn.Module):\n    def __init__(self, num_classes):\n        super(GroceryNetV2, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc = nn.Linear(2304, num_classes)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = self.pool(F.relu(self.conv4(x)))\n        x = self.pool(F.relu(self.conv5(x)))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x","metadata":{"id":"reUTGREk9vRO","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:55.488784Z","iopub.execute_input":"2025-05-25T10:54:55.489027Z","iopub.status.idle":"2025-05-25T10:54:55.504865Z","shell.execute_reply.started":"2025-05-25T10:54:55.489005Z","shell.execute_reply":"2025-05-25T10:54:55.504171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## \\> Step 3: Batch Normalization + 1 FC Layer + Global Average Pooling\n\n- Batch Normalization was applied to improve network stability and convergence. While the impact is limited due to the small batch size (which can cause fluctuations), it still helps the model converge more reliably.\n- An additional Fully Connected Layer was added to the classifier. This expansion aims to enhance the model's capacity to generalize by allowing it to learn more complex features through multiple network stages.\n- Global Average Pooling was used before the FC layers, reducing the number of parameters. This approach not only simplifies the model but also enhances generalization by focusing on feature importance rather than spatial details.","metadata":{"id":"YH1MEa41-KKj"}},{"cell_type":"code","source":"class GroceryNetV3(nn.Module):\n    def __init__(self, num_classes):\n        super(GroceryNetV3, self).__init__()\n\n        # Convolutional layers with batch normalization\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=1)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(32)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(64)\n        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn4 = nn.BatchNorm2d(128)\n        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.bn5 = nn.BatchNorm2d(256)\n        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(256, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n        x = self.global_avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x","metadata":{"id":"korFtFlk-hxU","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:55.505544Z","iopub.execute_input":"2025-05-25T10:54:55.505798Z","iopub.status.idle":"2025-05-25T10:54:55.526003Z","shell.execute_reply.started":"2025-05-25T10:54:55.505778Z","shell.execute_reply":"2025-05-25T10:54:55.525288Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## \\> Step 4: DropOut + Linear Learning Rate Scheduler\n\nTo reach closer to our target accuracy few steps were finally taken:\n\n- Dropout: Two Dropout layers have been added: one with a rate of 0.2 before the first fully connected layer and another with a rate of 0.4 before the final output layer. This should help mitigate overfitting by randomly deactivating neurons during training, forcing the network to learn more robust features rather than relying on specific activations.\n\n- Linear Learning Rate Scheduler was applied to gradually reduce the learning rate over the epochs. This approach helps maintain training stability, especially in later stages, by making smaller updates as the model starts to converge, thus avoiding overshooting minima in the loss landscape.","metadata":{"id":"EY-xEhQp-unM"}},{"cell_type":"code","source":"class GroceryNetV4(nn.Module):\n\n    def __init__(self, num_classes):\n        super(GroceryNetV4, self).__init__()\n\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=1,  stride=1)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(32)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(64)\n        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn4 = nn.BatchNorm2d(128)\n        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.bn5 = nn.BatchNorm2d(256)\n\n        # Global average pooling\n        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n\n        # FC layers\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout1 = nn.Dropout(0.2)\n        self.fc1 = nn.Linear(256, 512)\n        self.dropout2 = nn.Dropout(0.4)\n        self.fc2 = nn.Linear(512, num_classes)\n\n\n    def forward(self, x):\n        # Convolutional layers\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n\n        # GAP\n        x = self.global_avg_pool(x)\n\n        # Flatten before the FC layers\n        x = x.view(x.size(0), -1)\n\n        # Fully connected layers\n        x = self.dropout1(x)\n        x = F.relu(self.fc1(x))\n        x = self.dropout2(x)\n        x = self.fc2(x)\n\n        return x","metadata":{"id":"t4ply363-1wM","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:55.529096Z","iopub.execute_input":"2025-05-25T10:54:55.529531Z","iopub.status.idle":"2025-05-25T10:54:55.546511Z","shell.execute_reply.started":"2025-05-25T10:54:55.529511Z","shell.execute_reply":"2025-05-25T10:54:55.545762Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## \\> Pre-Processing\n\nData Augmentation and some image pre-processing is added before training each model. Although not having a remarkable impact in the overall accuracy scores it still helped in reaching the goal of 60% validation accuracy.\n\nThe following steps were taken to optimize image quality for improved model training:\n\n- **Random Flipping**:  \n\n  Applying random flips on both horizontal and vertical axes provided the model with varied perspectives of the images. This augmentation strategy helped prevent the model from overfitting on specific orientations, leading to improved generalization on unseen data and a noticeable rise in validation accuracy.\n\n- **Image Sharpening**:  \n\n  Since part of the images in the dataset suffered from blur, sharpening was applied as a preprocessing step to enhance edge clarity and make objects more distinct. This step was essential in helping the model capture finer details that could otherwise be lost in blurry images, thus significantly boosting recognition accuracy.\n\n- **Contrast Enhancement**:  \n\n  Increasing the contrast of the images helped in distinguishing objects more effectively by enhancing visual differences between regions. With sharper contrasts, the model was better able to recognize key features, which further contributed to improved generalization.","metadata":{"id":"u_PSWXXgzCUI"}},{"cell_type":"markdown","source":"## Train Models","metadata":{"id":"Ra65IsWs21OK"}},{"cell_type":"markdown","source":"### Network Parameters and Rationale\n\n- **Batch Size** (16):<br/>\n    - Small batch size consumes less resources but has a less smooth convergence, while acting as reguralizer factor.\n<br/>\n- **Early Stopping** (`Patience=5`):<br/>\n  - Prevents overfitting by halting training if validation performance plateaus for 5 epochs.\n<br/>\n- **Learning Rate** (`initial_lr=0.001`) and Optimizer (Adam):<br/>\n  - Adam optimizer is suitable for adaptive learning rates.\n  - The learning rate of 0.001 is commonly effective for Adam, providing a good balance of convergence speed and stability.\n<br/>\n- **Learning Rate Scheduler** (`LinearLR`):<br/>\n  - Gradually decreases learning rate from `1.0` to `0.00001` over all epochs, encouraging the model to stabilize in later stages for optimal final performance. This is used in `GroceryNetV5` model only.","metadata":{"id":"96hnLpBD0oVq"}},{"cell_type":"code","source":"# Early stopping class\nclass EarlyStopping:\n    def __init__(self, patience=5, verbose=False, measure_loss=True):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.measure_loss = measure_loss # defines if using val_loss or val_accuracy\n\n    def __call__(self, score):\n        if self.best_score is None:\n            self.best_score = score\n        elif self.measure_loss == True and (score > self.best_score):\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        elif self.measure_loss == False and (score < self.best_score):\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.counter = 0","metadata":{"id":"HRA0caexQdBP","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:55.547301Z","iopub.execute_input":"2025-05-25T10:54:55.547551Z","iopub.status.idle":"2025-05-25T10:54:55.565901Z","shell.execute_reply.started":"2025-05-25T10:54:55.54753Z","shell.execute_reply":"2025-05-25T10:54:55.565209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train model\ndef train_model_custom(model, train_loader, val_loader, num_epochs, criterion, scheduler, optimizer, early_stopping):\n\n    # Initialize data structs to store losses and accuracies\n  train_losses = []\n  val_losses = []\n  train_accuracies = []\n  val_accuracies = []\n  num_epochs_run = 0\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n  for epoch in range(num_epochs):\n\n      # Training phase\n      model.train()\n      correct_train = 0\n      total_train = 0\n      running_loss = 0.0\n\n      for images, labels in train_loader:\n          images, labels = images.to(device), labels.to(device)\n          optimizer.zero_grad()  # Clear gradients\n          outputs = model.forward(images)  # Forward pass\n          loss = criterion(outputs, labels)  # Calculate loss\n          loss.backward()  # Backpropagation\n          optimizer.step()  # Optimization step\n          running_loss += loss.item() * images.size(0)\n\n          _, predicted = torch.max(outputs, 1)\n\n          correct_train += (predicted == labels).sum().item()\n          total_train += labels.size(0)\n\n      train_loss = running_loss / len(train_loader.dataset)\n      train_accuracy = correct_train / total_train\n      train_losses.append(train_loss)\n      train_accuracies.append(train_accuracy)\n\n      # Validation phase\n      model.eval()  # Set model to evaluation mode\n      correct_val = 0\n      total_val = 0\n      val_running_loss = 0.0\n\n      with torch.no_grad():  # No need to track gradients during validation\n          for images, labels in val_loader:\n              images, labels = images.to(device), labels.to(device)\n\n              outputs = model(images)\n              loss = criterion(outputs, labels)\n\n              val_running_loss += loss.item() * images.size(0)\n              _, predicted = torch.max(outputs, 1)\n              correct_val += (predicted == labels).sum().item()\n              total_val += labels.size(0)\n\n      val_loss = val_running_loss / len(val_loader.dataset)\n      val_accuracy = correct_val / total_val\n      val_losses.append(val_loss)\n      val_accuracies.append(val_accuracy)\n\n      # Print the statistics\n      print(f'Epoch [{epoch+1}/{num_epochs}] - Loss: {train_loss:.4f} - Accuracy: {train_accuracy:.4f} - Val_loss: {val_loss:.4f}, Val_accuracy: {val_accuracy:.4f}')\n\n      # Increase epoch\n      num_epochs_run = num_epochs_run + 1\n\n      # lr update\n      if scheduler != None:\n          scheduler.step()\n\n      # Check early stopping\n      early_stopping(val_loss)\n      if early_stopping.early_stop:\n          print(\"Early stopping triggered.\")\n          break\n\n  return train_losses, val_losses, train_accuracies, val_accuracies, num_epochs_run","metadata":{"id":"_vsqDuY4q-go","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:55.566599Z","iopub.execute_input":"2025-05-25T10:54:55.566861Z","iopub.status.idle":"2025-05-25T10:54:55.584567Z","shell.execute_reply.started":"2025-05-25T10:54:55.56684Z","shell.execute_reply":"2025-05-25T10:54:55.583865Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"id":"s_QVe7g6SAc7"}},{"cell_type":"code","source":"# Training params\nnum_epochs = 30\ninitial_lr = 0.001\nimage_size = 128\npatience = 5\nbatch_size = 16","metadata":{"id":"h94eA3BOSAc7","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:55.585174Z","iopub.execute_input":"2025-05-25T10:54:55.58539Z","iopub.status.idle":"2025-05-25T10:54:55.604982Z","shell.execute_reply.started":"2025-05-25T10:54:55.585374Z","shell.execute_reply":"2025-05-25T10:54:55.604223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Transformations for training\ndef get_train_transform():\n    return transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.RandomRotation(30),\n        transforms.RandomAdjustSharpness(0.5, p=0.5),\n        transforms.RandomAutocontrast(p=0.5),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize based on ImageNet mean/std\n    ])\n\n# Transformations for validation\ndef get_validation_transform():\n    return transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize based on ImageNet mean/std\n    ])","metadata":{"id":"0_2hLXMaSvU2","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:55.606042Z","iopub.execute_input":"2025-05-25T10:54:55.606294Z","iopub.status.idle":"2025-05-25T10:54:55.619725Z","shell.execute_reply.started":"2025-05-25T10:54:55.60627Z","shell.execute_reply":"2025-05-25T10:54:55.619029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare model with data\ndef get_model_version(version):\n    print('-'*30)\n    print(f'Training model: GroceryNet{version}')\n    print('-'*30)\n\n    # Initialize datasets\n    train_dataset = GroceryStoreDataset(split=\"train\", transform=get_train_transform())\n    val_dataset = GroceryStoreDataset(split=\"val\", transform=get_validation_transform())\n\n    # Number of classes\n    num_classes = train_dataset.get_num_classes()\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    # Early stopping\n    early_stopping = EarlyStopping(patience=patience)\n\n    # Versioned model\n    model = version_map[version](num_classes=num_classes)\n\n    # Optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n\n    # Learning rate scheduler (only V4)\n    if version == 'V4':\n        scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.00001, total_iters=num_epochs)\n    else:\n        scheduler = None\n\n    # Move model to GPU if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Define loss function\n    criterion = nn.CrossEntropyLoss()\n\n    return model, train_loader, val_loader, num_epochs, criterion, scheduler, optimizer, early_stopping","metadata":{"id":"xTAvy2jM3IfR","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:55.620586Z","iopub.execute_input":"2025-05-25T10:54:55.62087Z","iopub.status.idle":"2025-05-25T10:54:55.640595Z","shell.execute_reply.started":"2025-05-25T10:54:55.620848Z","shell.execute_reply":"2025-05-25T10:54:55.639876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reset_seeds(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)","metadata":{"id":"ulz8MngwNfml","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:55.641343Z","iopub.execute_input":"2025-05-25T10:54:55.641518Z","iopub.status.idle":"2025-05-25T10:54:55.659671Z","shell.execute_reply.started":"2025-05-25T10:54:55.641505Z","shell.execute_reply":"2025-05-25T10:54:55.659008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mapping function to different model versions (v1-v5)\nmodel_versions  = [\n    'V1',\n    'V2',\n    'V3',\n    'V4']\n\nversion_map = {\n    'V1': GroceryNetV1,\n    'V2': GroceryNetV2,\n    'V3': GroceryNetV3,\n    'V4': GroceryNetV4\n}\n\n# Data structures for plotting\ntrain_accuracy_all_models = []\nval_accuracy_all_models = []\ntrain_loss_all_models = []\nval_loss_all_models = []\nnum_epochs_run_all_models = []\n\nfor i, version in enumerate(model_versions):\n    reset_seeds(i)\n\n    # Get model\n    model, train_loader, val_loader, num_epochs, criterion, scheduler, optimizer, early_stopping = get_model_version(version)\n\n    # Train model\n    train_losses, val_losses, train_accuracies, val_accuracies, num_epochs_run = train_model_custom(model, train_loader, val_loader, num_epochs, criterion, scheduler, optimizer, early_stopping)\n\n    # Store results for plotting\n    train_accuracy_all_models.append(train_accuracies)\n    val_accuracy_all_models.append(val_accuracies)\n    train_loss_all_models.append(train_losses)\n    val_loss_all_models.append(val_losses)\n    num_epochs_run_all_models.append(num_epochs_run)","metadata":{"id":"1yZaDhBXSAc7","outputId":"1f95dad5-634a-48f6-bd7e-a3abf77a4a73","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:54:55.660366Z","iopub.execute_input":"2025-05-25T10:54:55.660606Z","iopub.status.idle":"2025-05-25T11:05:06.492374Z","shell.execute_reply.started":"2025-05-25T10:54:55.660565Z","shell.execute_reply":"2025-05-25T11:05:06.491563Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Plot Models Accuracy and Loss\n\n","metadata":{"id":"MGDweUo9zMHP"}},{"cell_type":"code","source":"# Plotting the results\n\nfor i in range(4):\n\n    epochs = range(1, num_epochs_run_all_models[i] + 1)\n\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_accuracy_all_models[i], 'b', label='Training')\n    plt.plot(epochs, val_accuracy_all_models[i], 'r', label='Validation')\n    plt.title(f'Accuracy (GroceryNetV{i+1})')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_loss_all_models[i], 'b', label='Training')\n    plt.plot(epochs, val_loss_all_models[i], 'r', label='Validation')\n    plt.title(f'Loss (GroceryNetV{i+1})')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.subplots_adjust(wspace=0.2)\n    plt.show()\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nfor i in range(4):\n    epochs = range(1, num_epochs_run_all_models[i] + 1)\n    plt.plot(epochs, train_accuracy_all_models[i], label=f\"v{i+1}\")\nplt.title(f'Train Accuracy (All Models)')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\n\nplt.subplot(1, 2, 2)\nfor i in range(4):\n    epochs = range(1, num_epochs_run_all_models[i] + 1)\n    plt.plot(epochs, val_accuracy_all_models[i], label=f\"v{i+1}\")\nplt.title(f'Validation Accuracy (All Models)')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"id":"nG_37J_-3V0l","outputId":"1edc1b5b-5132-47b1-cf8e-228c6ac193ae","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:06.493257Z","iopub.execute_input":"2025-05-25T11:05:06.493583Z","iopub.status.idle":"2025-05-25T11:05:08.482545Z","shell.execute_reply.started":"2025-05-25T11:05:06.49356Z","shell.execute_reply":"2025-05-25T11:05:08.481895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summary of Improvements\n\n### GroceryNetV1\n\nServes as the baseline model, using a simple convolutional architecture. It achieves an average validation accuracy of 32%, peaking at 40% before plateauing. The model's shallow design limits its ability to capture complex features, leading to early signs of overfitting. The rapid decline in validation accuracy after early epochs suggests that more sophisticated network designs are necessary for better generalization.\n\n### GroceryNetV2\n\nImproves upon V1 by introducing more convolutional layers, enhancing its ability to learn intricate features. It achieves an average validation accuracy of 38%, with a peak of 46%. While deeper layers contribute to improved feature extraction, overfitting remains a challenge, indicating the need for additional regularization methods to sustain higher performance across epochs.\n\n### GroceryNetV3\n\nAdds Batch Normalization and Global Average Pooling (GAP), significantly boosting generalization. It achieves an average validation accuracy of 51%, peaking at 55%. Batch Normalization stabilizes learning, while GAP reduces overfitting by minimizing the number of parameters. These enhancements demonstrate the effectiveness of architectural modifications in improving model robustness.\n\n### GroceryNetV4\n\nIncorporates Dropout layers with rates of 0.2 and 0.4, further improving regularization. It achieves an average validation accuracy of 60%, peaking at 66%. The use of Dropout effectively mitigates overfitting, resulting in the highest accuracy across all versions. This model highlights the importance of balancing model complexity with appropriate regularization techniques.\n\n\n| Model | Architecture changes | Validation Accuracy | Train Accuracy |\n|---|---|---|---|\n|GroceryNetV1|\tSimple CNN baseline|\t~32%|\t~59%|\n|GroceryNetV2|\tDeeper network with 4 extra conv layers|\t~42%|\t~70%|\n|GroceryNetV3|\tBatch Normalization + GAP|\t~51%|\t~71%|\n|GroceryNetV4|\tGAP + Dropout + LR Scheduler|\t~61%|\t~83%|","metadata":{"id":"2oiHiDlk_PWC"}},{"cell_type":"markdown","source":"# Part 2: Fine-Tune ResNet18\n\nFine tuning ResNet18 keeping GroceryNetV4 parameters fixed.","metadata":{"id":"k34ktrSERp4J"}},{"cell_type":"code","source":"def train_model_resnet18(early_stop, num_epochs, model, train_loader, val_loader, optimizer, scheduler, criterion, device, early_stopping):\n\n  num_epochs_run = 0\n  val_accuracy = 0.0\n  val_loss = 0.0\n  train_accuracy = 0.0\n  train_loss = 0.0\n  train_losses = []\n  val_losses = []\n  train_accuracies = []\n  val_accuracies = []\n\n  for epoch in range(num_epochs):\n      model.train()  # Set model to training mode\n      correct_train = 0\n      total_train = 0\n      running_loss = 0.0\n      val_running_loss = 0.0\n\n      for images, labels in train_loader:\n          images, labels = images.to(device), labels.to(device)\n\n          # Forward pass\n          outputs = model(images)\n          loss = criterion(outputs, labels)\n\n          # Backward pass and optimization\n          optimizer.zero_grad()\n\n          loss.backward()\n          optimizer.step()\n\n          running_loss += loss.item() * images.size(0)\n          _, predicted = torch.max(outputs, 1)\n          correct_train += (predicted == labels).sum().item()\n          total_train += labels.size(0)\n\n      train_loss = running_loss / len(train_loader.dataset)\n      train_accuracy = correct_train / total_train\n      train_losses.append(train_loss)\n      train_accuracies.append(train_accuracy)\n\n      # Validation phase\n      model.eval()\n      all_preds = []\n      all_labels = []\n      with torch.no_grad():\n          for images, labels in val_loader:\n              images, labels = images.to(device), labels.to(device)\n              outputs = model(images)\n              loss = criterion(outputs, labels)\n              val_running_loss += loss.item() * images.size(0)\n              _, preds = torch.max(outputs, 1)\n              all_preds.extend(preds.cpu().numpy())\n              all_labels.extend(labels.cpu().numpy())\n\n      val_loss = val_running_loss / len(val_loader.dataset)\n      val_accuracy = accuracy_score(all_labels, all_preds)\n      val_losses.append(val_loss)\n      val_accuracies.append(val_accuracy)\n\n      print(f'Epoch [{epoch+1}/{num_epochs}] - Loss: {train_loss:.4f} - Accuracy: {train_accuracy:.4f} - Val_loss: {val_loss:.4f}, Val_accuracy: {val_accuracy:.4f}')\n\n      num_epochs_run = num_epochs_run + 1\n\n      # lr update\n      if scheduler != None:\n        scheduler.step()\n\n      # Check early stopping\n      if early_stop:\n        early_stopping(val_accuracy) # use validation accuracy as metric for ES, as we want to maximize it\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered.\")\n            break\n\n  return model, train_losses, val_losses, train_accuracies, val_accuracies, num_epochs_run","metadata":{"id":"ThGxKGNMPQ6_","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:08.483357Z","iopub.execute_input":"2025-05-25T11:05:08.483614Z","iopub.status.idle":"2025-05-25T11:05:08.492163Z","shell.execute_reply.started":"2025-05-25T11:05:08.483579Z","shell.execute_reply":"2025-05-25T11:05:08.491632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_parameter_requires_grad(model, feature_extracting):\n  params_to_update = list(model.parameters())\n  if feature_extracting:\n      params_to_update = []\n      for name,param in model.named_parameters():\n          if param.requires_grad == True:\n              params_to_update.append(param)\n  print(f\"Params to learn: {len(list(params_to_update))}\")\n  return params_to_update","metadata":{"id":"UVj-7kMyQHX-","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:08.492754Z","iopub.execute_input":"2025-05-25T11:05:08.493213Z","iopub.status.idle":"2025-05-25T11:05:08.513787Z","shell.execute_reply.started":"2025-05-25T11:05:08.493188Z","shell.execute_reply":"2025-05-25T11:05:08.513092Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## \\> Step 1: Fine-Tune ResNet18 with baseline model parameters.\n\nParameters inherited from custom model:\n- `image_size` = 128\n- `batch_size` = 16\n- `num_epochs` = 30\n- `patience` = 5\n- `optimizer` = `Adam`\n- `LR scheduler` = `Linear (1.0 to 0.0001)`\n- `Data Augmentation`:\n  -  `RandomHorizontalFlip`\n  -  `RandomVerticalFlip`\n  -  `RandomRotation`\n  -  `RandomAdjustSharpness`\n  -  `RandomAutocontrast`","metadata":{"id":"VK_m6_gGNoHY"}},{"cell_type":"code","source":"# Model params\nimage_size = 128\nbatch_size = 16\nnum_epochs = 30\npatience = 5\n\nreset_seeds(42)\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define transformations for the training and validation sets\ntrain_transform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Initialize datasets\ntrain_dataset = GroceryStoreDataset(split=\"train\", transform=train_transform)\nval_dataset = GroceryStoreDataset(split=\"val\", transform=val_transform)\n\n# Data Loaders\ntrain_loader_resnet_1 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader_resnet_1 = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n# Load pre-trained ResNet-18 model\nmodel_resnet_1 = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_resnet_1.to(device)\n\n# Modify the last layer to match the number of classes in the GroceryStoreDataset\nnum_classes = train_dataset.get_num_classes()\nmodel_resnet_1.fc = nn.Linear(model_resnet_1.fc.in_features, num_classes)\n\n# Move the model to the device\nmodel_resnet_1 = model_resnet_1.to(device)\n\n# Early stopping\nearly_stopping = EarlyStopping(patience=patience, measure_loss=False)","metadata":{"id":"_POuqjGuXi4u","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:08.514391Z","iopub.execute_input":"2025-05-25T11:05:08.514624Z","iopub.status.idle":"2025-05-25T11:05:09.123117Z","shell.execute_reply.started":"2025-05-25T11:05:08.514608Z","shell.execute_reply":"2025-05-25T11:05:09.12235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fine tuning\n\nprint('Fine Tuning Resnet-18...')\nparams_to_update = set_parameter_requires_grad(model_resnet_1, feature_extracting=False)\noptimizer = torch.optim.Adam(model_resnet_1.parameters(), lr=initial_lr)\nscheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0001, total_iters=num_epochs)\nmodel_resnet_1, train_losses_resnet_1, val_losses_resnet_1, train_accuracies_resnet_1, val_accuracies_resnet_1, num_epochs_run_resnet_1 = train_model_resnet18(True, num_epochs, model_resnet_1, train_loader_resnet_1, val_loader_resnet_1, optimizer, scheduler, criterion, device, early_stopping)","metadata":{"id":"WA3scbxGBBaN","outputId":"96954501-789c-46d2-e159-92279736202b","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:09.123927Z","iopub.execute_input":"2025-05-25T11:05:09.124227Z","iopub.status.idle":"2025-05-25T11:07:13.918534Z","shell.execute_reply.started":"2025-05-25T11:05:09.124201Z","shell.execute_reply":"2025-05-25T11:07:13.9177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting the results\nepochs = range(1, num_epochs_run_resnet_1 + 1)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_accuracies_resnet_1, 'b', label='Training Accuracy')\nplt.plot(epochs, val_accuracies_resnet_1, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Plot loss\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_losses_resnet_1, 'b', label='Training Loss')\nplt.plot(epochs, val_losses_resnet_1, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"id":"cG3XYwRU_3v7","outputId":"2ee188d6-c326-43ed-c1f8-0a3801d4aecb","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:07:13.919483Z","iopub.execute_input":"2025-05-25T11:07:13.919849Z","iopub.status.idle":"2025-05-25T11:07:14.257398Z","shell.execute_reply.started":"2025-05-25T11:07:13.919824Z","shell.execute_reply":"2025-05-25T11:07:14.256687Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## \\> Step 2: Fine-Tune ResNet-18 with updated parameters\n\n### Pramaters updates and motivation\n\n1. **Image Size: 128 → 224**  \n   Matching the input size to 224 (ResNet-18's original ImageNet size) allows the model to utilize its pre-trained weights more effectively, capturing finer details and improving accuracy.\n\n2. **Batch Size: 16 → 128**  \n   A larger batch size provides a more stable gradient estimate, leading to smoother training dynamics and faster convergence.\n\n3. **Patience: 5 → 3**  \n   No need for extra patience in the early stopping config as the updated params make the model converge much faster.\n\n4. **Optimizer: Adam → SGD with Momentum (0.99)**  \n   Switching to SGD with high momentum enables more controlled, stable updates, reducing oscillations and improving convergence stability for fine-tuning.\n\n5. **Learning Rate Scheduler: Linear Decay → None**  \n   Removing the scheduler allows the learning rate to stay consistent, supporting a steady convergence without the potential instability from early learning rate reductions.\n\n6. **Data Augmentation: RandomHorizontalFlip**  \n   Limiting augmentation to RandomHorizontalFlip reduces noise while still promoting some robustness, allowing the model to converge faster on dataset-specific features.\n\n### Parameters list:\n- `image_size` = 224\n- `batch_size` = 128\n- `num_epochs` = 30\n- `patience` = 5\n- `optimizer` = `SGD with momentum (0.99)`\n- `LR scheduler` = `none`\n- `Data Augmentation`:\n  -  `RandomHorizontalFlip`","metadata":{"id":"A0_glXIwXN2A"}},{"cell_type":"code","source":"# Model params\nimage_size = 224\nbatch_size = 128\nnum_epochs = 30\npatience = 3\n\n# Define transformations for the training and validation sets\ntrain_transform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Initialize datasets\ntrain_dataset = GroceryStoreDataset(split=\"train\", transform=train_transform)\nval_dataset = GroceryStoreDataset(split=\"val\", transform=val_transform)\n\n# Data Loaders\ntrain_loader_resnet_2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader_resnet_2 = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n# Load pre-trained ResNet-18 model\nmodel_resnet_2 = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_resnet_2.to(device)\n\n# Modify the last layer to match the number of classes in the GroceryStoreDataset\nnum_classes = train_dataset.get_num_classes()\nmodel_resnet_2.fc = nn.Linear(model_resnet_2.fc.in_features, num_classes)\n\n# Move the model to the device\nmodel_resnet_2 = model_resnet_2.to(device)\n\n# Early stopping via validationa accuracy\nearly_stopping = EarlyStopping(patience=patience, measure_loss=False)","metadata":{"id":"wY_T2ejMXcXM","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:07:14.2583Z","iopub.execute_input":"2025-05-25T11:07:14.258903Z","iopub.status.idle":"2025-05-25T11:07:14.476375Z","shell.execute_reply.started":"2025-05-25T11:07:14.258885Z","shell.execute_reply":"2025-05-25T11:07:14.475849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fine tuning\nprint('Fine Tuning Resnet-18...')\nreset_seeds(90)\nparams_to_update = set_parameter_requires_grad(model_resnet_2, feature_extracting=False)\noptimizer = torch.optim.SGD(params_to_update, lr=0.001, momentum=0.99)\nmodel_resnet_2, train_losses_resnet_2, val_losses_resnet_2, train_accuracies_resnet_2, val_accuracies_resnet_2, num_epochs_run_resnet_2 = train_model_resnet18(True, num_epochs, model_resnet_2, train_loader_resnet_2, val_loader_resnet_2, optimizer, None, criterion, device, early_stopping)","metadata":{"id":"h7KM5i15XSvy","outputId":"dadcdc39-b13f-4c40-a941-d47b732e1750","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:07:14.477029Z","iopub.execute_input":"2025-05-25T11:07:14.477201Z","iopub.status.idle":"2025-05-25T11:11:40.367266Z","shell.execute_reply.started":"2025-05-25T11:07:14.477188Z","shell.execute_reply":"2025-05-25T11:11:40.366588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting the results\nepochs = range(1, num_epochs_run_resnet_2 + 1)\n\n# accuracy\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_accuracies_resnet_2, 'b', label='Training Accuracy')\nplt.plot(epochs, val_accuracies_resnet_2, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# loss\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_losses_resnet_2, 'b', label='Training Loss')\nplt.plot(epochs, val_losses_resnet_2, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"id":"411Lfa9QkwgY","outputId":"e021cd8c-c419-429a-abe3-1cc79a23c904","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:11:40.36805Z","iopub.execute_input":"2025-05-25T11:11:40.368268Z","iopub.status.idle":"2025-05-25T11:11:40.71265Z","shell.execute_reply.started":"2025-05-25T11:11:40.368252Z","shell.execute_reply":"2025-05-25T11:11:40.712142Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The End","metadata":{"id":"5NRH17-cSAdA"}}]}